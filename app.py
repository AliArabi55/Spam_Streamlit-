
import os
import subprocess
import sys

def check_and_install_requirements():
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙˆØªØ«Ø¨ÙŠØªÙ‡Ø§ Ù…Ù† Ù…Ù„Ù requirements.txt"""
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…Ø¹ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ØµØ­ÙŠØ­Ø©
    required_packages = {
        'streamlit': 'streamlit',
        'joblib': 'joblib', 
        'scikit-learn': 'sklearn',
        'pandas': 'pandas',
        'numpy': 'numpy'
    }
    
    missing_packages = []
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒÙ„ Ù…ÙƒØªØ¨Ø©
    for package_name, import_name in required_packages.items():
        try:
            __import__(import_name)
            print(f"âœ… {package_name} Ù…ÙˆØ¬ÙˆØ¯Ø©")
        except ImportError:
            print(f"âš ï¸ {package_name} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©")
            missing_packages.append(package_name)
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ù…ÙƒØªØ¨Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©ØŒ Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØªÙ‡Ø§ Ù…Ù† requirements.txt
    if missing_packages:
        print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©: {', '.join(missing_packages)}")
        requirements_file = "requirements.txt"
        
        if os.path.exists(requirements_file):
            try:
                # ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ù…Ù† requirements.txt
                subprocess.check_call([
                    sys.executable, "-m", "pip", "install", "-r", requirements_file, "--quiet"
                ])
                print("âœ… ØªÙ… ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
                return True
            except subprocess.CalledProcessError as e:
                print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª: {e}")
                return False
        else:
            print("âš ï¸ Ù…Ù„Ù requirements.txt ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            return False
    else:
        print("âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©!")
        return True

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª ÙˆØªØ«Ø¨ÙŠØªÙ‡Ø§ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
print("ğŸ” Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©...")
installation_success = check_and_install_requirements()

import streamlit as st
import joblib
from styles import apply_custom_styles, configure_page
from languages import LANGUAGES

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø·
configure_page()
apply_custom_styles()

# Ø±Ø³Ø§Ù„Ø© Ø­Ø§Ù„Ø© ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
if installation_success:
    st.success("âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…ØªÙˆÙØ±Ø© ÙˆÙ…Ø­Ø¯Ø«Ø©!")
else:
    st.error("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªØ«Ø¨ÙŠØª Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª. ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØªÙ‡Ø§ ÙŠØ¯ÙˆÙŠØ§Ù‹.")
    st.code("pip install -r requirements.txt", language="bash")

# Initialize session state for language
if 'language' not in st.session_state:
    st.session_state.language = 'English'

# Load the saved model and vectorizer
@st.cache_resource
def load_models():
    model = joblib.load("spam_classifier.pkl")
    vectorizer = joblib.load("vectorizer.pkl")
    return model, vectorizer

model, vectorizer = load_models()

# Get current language texts
texts = LANGUAGES[st.session_state.language]

# Main header
st.markdown(f'<h1 class="main-header">{texts["main_header"]}</h1>', unsafe_allow_html=True)
st.markdown(f'<p class="sub-header">{texts["sub_header"]}</p>', unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    # Language toggle
    st.markdown(f"### {texts['language_toggle']}")
    language_options = ["English", "Deutsch"]
    selected_language = st.selectbox(
        "",
        language_options,
        index=language_options.index(st.session_state.language),
        key="language_selector"
    )
    
    # Update language if changed
    if selected_language != st.session_state.language:
        st.session_state.language = selected_language
        st.rerun()
    
    st.markdown("---")
    
    # Model Accuracy Section
    st.markdown(f"""
    <div class="accuracy-box">
        <h3>{texts["model_accuracy"]}</h3>
        <p>{texts["model_accuracy_desc"]}</p>
    </div>
    """, unsafe_allow_html=True)

# Main content area
col1, col2 = st.columns([3, 2])

with col1:
    # Input section
    st.markdown(f"## {texts['enter_message']}")
    user_input = st.text_area(
        "",
        height=150,
        placeholder=texts["message_placeholder"],
        key="message_input"
    )
    
    # Analyze button
    analyze_button = st.button(texts["analyze_button"], use_container_width=True, type="primary")
    
    # Prediction logic
    if analyze_button:
        if user_input.strip() == "":
            st.warning(texts["enter_message_warning"])
        else:
            with st.spinner(texts["analyzing"]):
                # Transform the input text using the vectorizer
                vec = vectorizer.transform([user_input])
                # Predict using the model
                pred = model.predict(vec)[0]
                probability = model.predict_proba(vec)[0]
                
                # Display results
                st.markdown("---")
                st.markdown(f"### {texts['analysis_results']}")
                
                if pred == 1:
                    st.markdown(f"""
                    <div class="result-spam">
                        {texts["spam_warning"]} 
                        <br><strong>{texts["confidence_level"]}: {probability[1]:.1%}</strong>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    st.markdown(f"""
                    ### {texts["spam_tips"]}
                    {texts["spam_tips_list"]}
                    """)
                else:
                    st.markdown(f"""
                    <div class="result-ham">
                        {texts["ham_result"]} 
                        <br><strong>{texts["confidence_level"]}: {probability[0]:.1%}</strong>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    st.markdown(f"""
                    ### {texts["ham_tips"]}
                    {texts["ham_tips_list"]}
                    """)
                
                # Additional analysis
                st.markdown("---")
                col_stats1, col_stats2, col_stats3 = st.columns(3)
                
                with col_stats1:
                    st.metric(texts["word_count"], len(user_input.split()))
                
                with col_stats2:
                    st.metric(texts["char_count"], len(user_input))
                
                with col_stats3:
                    confidence = max(probability[0], probability[1])
                    st.metric(texts["confidence_level"], f"{confidence:.1%}")

with col2:
    # Examples section
    st.markdown(f"## {texts['try_examples']}")
    
    # Ham Examples
    with st.expander(texts["ham_examples"], expanded=False):
        st.markdown(f"""
        <div class="example-box">
            <pre style="color: #2c3e50; font-family: 'Segoe UI', Arial, sans-serif; white-space: pre-wrap; margin: 0;">{texts["ham_examples_list"]}</pre>
        </div>
        """, unsafe_allow_html=True)
    
    # Spam Examples
    with st.expander(texts["spam_examples"], expanded=False):
        st.markdown(f"""
        <div class="spam-example-box">
            <pre style="color: #721c24; font-family: 'Segoe UI', Arial, sans-serif; white-space: pre-wrap; margin: 0;">{texts["spam_examples_list"]}</pre>
        </div>
        """, unsafe_allow_html=True)

# Footer
st.markdown("---")
st.markdown(f"""
<div style="text-align: center; color: #666; padding: 2rem;">
    <p>{texts["footer_text"]}</p>
</div>
""", unsafe_allow_html=True)
